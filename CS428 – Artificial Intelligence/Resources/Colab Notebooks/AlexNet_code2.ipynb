{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOu6YrG6JmGfzosB8R5MaOh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This is the base AlexNet CNN code then I will list out the following modiciations I made to it in seperate code sections."],"metadata":{"id":"se_Tw3kD33PX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHcWSxDH3w3J"},"outputs":[],"source":["# Install & import necessary libraries\n","!pip install patool==1.12\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers, models\n","import patoolib\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Extract the zip file from Google Drive\n"," patoolib.extract_archive(\"/content/drive/MyDrive/Hood College/archive.zip\", outdir=\"/content/extracted_images\") if the drive is not connected to the program uncomment this line and specify the dir to the zip file or dataset\n","\n","# define the directory containing the dataset\n","data_dir = '/content/extracted_images/leapGestRecog'\n","\n","#load the dataset and split it into training and validation sets\n","train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    validation_split=0.3,  # reserve 30% for validation\n","    subset=\"training\",\n","    seed=123,\n","    image_size=(227, 227),  # resize images to 227x227 pixels (AlexNet input size)\n","    batch_size=32,\n","    label_mode='categorical'\n",")\n","\n","validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    validation_split=0.3,\n","    subset=\"validation\",\n","    seed=123,\n","    image_size=(227, 227),\n","    batch_size=32,\n","    label_mode='categorical'\n",")\n","\n","# Display class names to verify labels\n","class_names = train_dataset.class_names\n","print(\"Class Names:\", class_names)\n","\n","# Build the AlexNet model architecture\n","def build_alexnet(input_shape=(227, 227, 3), num_classes=len(class_names)):\n","    model = models.Sequential([\n","        layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Flatten(),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","    return model\n","\n","# Create the AlexNet model\n","alexnet_model = build_alexnet()\n","\n","# Compile the model\n","alexnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Print the model summary\n","alexnet_model.summary()\n","\n","# Train the model\n","history = alexnet_model.fit(\n","    train_dataset,\n","    validation_data=validation_dataset,\n","    epochs=5 # reduced epochs for testing purposes, increase to 10 to get better results.\n",")\n","\n","# evaluate the model on the validation set\n","val_loss, val_acc = alexnet_model.evaluate(validation_dataset, verbose=2)\n","print(f\"Validation accuracy: {val_acc:.4f}\")\n","\n","# plot training and validation accuracy\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy for AlexNet')\n","plt.show()\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"5f4nVzG532RM"}},{"cell_type":"markdown","source":["Testing different batch sizes"],"metadata":{"id":"-31-5PgB35Xy"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install patool==1.12\n","\n","# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers, models\n","import patoolib\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Extract the zip file from Google Drive\n","#patoolib.extract_archive(\"/content/drive/MyDrive/Hood College/archive.zip\", outdir=\"/content/extracted_images\")\n","\n","# Define the directory containing the dataset\n","data_dir = '/content/extracted_images/leapGestRecog'\n","\n","# Function to load dataset with a specified batch size\n","def load_dataset(batch_size):\n","    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.3,\n","        subset=\"training\",\n","        seed=123,\n","        image_size=(227, 227),\n","        batch_size=batch_size,\n","        label_mode='categorical'\n","    )\n","\n","    validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.3,\n","        subset=\"validation\",\n","        seed=123,\n","        image_size=(227, 227),\n","        batch_size=batch_size,\n","        label_mode='categorical'\n","    )\n","    return train_dataset, validation_dataset\n","\n","# Build AlexNet model with Batch Normalization applied for more consistant results\n","def build_alexnet_with_bn(input_shape=(227, 227, 3), num_classes=10):\n","    model = models.Sequential([\n","        layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Flatten(),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","    return model\n","\n","# Define a function to train the model with different batch sizes\n","def train_with_batch_size(batch_size):\n","    print(f\"\\nTraining with Batch Size = {batch_size}\\n\")\n","\n","    # load the dataset with the given batch size\n","    train_dataset, validation_dataset = load_dataset(batch_size)\n","\n","\n","    alexnet_model = build_alexnet_with_bn()\n","\n","    # compile the model\n","    alexnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    # train the model\n","    history = alexnet_model.fit(\n","        train_dataset,\n","        validation_data=validation_dataset,\n","        epochs=5 # reduced to 5 for testing purpose increase t o10 for better results\n","    )\n","\n","    #evaluate the model on the validation set\n","    val_loss, val_acc = alexnet_model.evaluate(validation_dataset, verbose=2)\n","    print(f\"Validation accuracy for Batch Size = {batch_size}: {val_acc:.4f}\")\n","\n","    # Plot the training and validation accuracy\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.title(f'Training and Validation Accuracy for Batch Size = {batch_size}')\n","    plt.show()\n","\n","# Test different batch sizes and gather results\n","batch_sizes = [16, 32, 64, 128]\n","\n","for batch_size in batch_sizes:\n","    train_with_batch_size(batch_size)\n"],"metadata":{"id":"JDuG-3Sp37KL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing with data augmentation"],"metadata":{"id":"Z6CpK96Y3_fl"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install patool==1.12\n","\n","# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers, models\n","import patoolib\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Extract the zip file from Google Drive\n","# patoolib.extract_archive(\"/content/drive/MyDrive/Hood College/archive.zip\", outdir=\"/content/extracted_images\")\n","\n","# Define the directory containing the dataset\n","data_dir = '/content/extracted_images/leapGestRecog'\n","\n","# Function for loading dataset with data augmentation adjustments\n","def load_dataset_with_augmentation(batch_size):\n","    datagen = ImageDataGenerator(\n","        validation_split=0.3,\n","        rotation_range=20,  # rotate images up to 20 degrees\n","        horizontal_flip=True,  # flip images horizontally\n","        zoom_range=0.2  #scale up by 20%\n","    )\n","\n","    train_dataset = datagen.flow_from_directory(\n","        data_dir,\n","        target_size=(227, 227),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        subset='training',  # set as training data\n","        seed=123\n","    )\n","\n","    validation_dataset = datagen.flow_from_directory(\n","        data_dir,\n","        target_size=(227, 227),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        subset='validation',  # set as validation data\n","        seed=123\n","    )\n","    return train_dataset, validation_dataset\n","\n","# build AlexNet model\n","def build_alexnet_with_bn(input_shape=(227, 227, 3), num_classes=10):\n","    model = models.Sequential([\n","        layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Flatten(),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","    return model\n","\n","# Define a function to train the model with data augmentation\n","def train_with_data_augmentation(batch_size):\n","    print(f\"\\nTraining with Batch Size = {batch_size} and Data Augmentation (Flip, Rotate, Scale)\\n\")\n","\n","\n","    train_dataset, validation_dataset = load_dataset_with_augmentation(batch_size)\n","\n","    # create the AlexNet model\n","    alexnet_model = build_alexnet_with_bn()\n","\n","    # compile the model\n","    alexnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    # train the model\n","    history = alexnet_model.fit(\n","        train_dataset,\n","        validation_data=validation_dataset,\n","        epochs=5  # Adjust the number of epochs as needed (decresed from 10 to 5 for testing)\n","    )\n","\n","    #evaluate the model on the validation set\n","    val_loss, val_acc = alexnet_model.evaluate(validation_dataset, verbose=2)\n","    print(f\"Validation accuracy for Batch Size = {batch_size} with Data Augmentation: {val_acc:.4f}\")\n","\n","    # plot the training and validation accuracy\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.title(f'Training and Validation Accuracy for Batch Size = {batch_size} with Data Augmentation')\n","    plt.show()\n","\n","# fixed batch size of 32\n","train_with_data_augmentation(batch_size=32)\n"],"metadata":{"id":"fQqdq2Jx4ADK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This section tests different learning rates for AlexNet [0.1, 0.01, 0.001, 0.0001]"],"metadata":{"id":"ZmZMFD2h4DU4"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install patool==1.12\n","\n","# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, optimizers\n","import matplotlib.pyplot as plt\n","import patoolib\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Extract the zip file from Google Drive\n","#patoolib.extract_archive(\"/content/drive/MyDrive/Hood College/archive.zip\", outdir=\"/content/extracted_images\")\n","\n","# define the directory containing the dataset\n","data_dir = '/content/extracted_images/leapGestRecog'\n","\n","# Function for loading the dataset\n","def load_dataset(batch_size):\n","    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.3,\n","        subset=\"training\",\n","        seed=123,\n","        image_size=(227, 227),\n","        batch_size=batch_size,\n","        label_mode='categorical'\n","    )\n","\n","    validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.3,\n","        subset=\"validation\",\n","        seed=123,\n","        image_size=(227, 227),\n","        batch_size=batch_size,\n","        label_mode='categorical'\n","    )\n","    return train_dataset, validation_dataset\n","\n","# Build AlexNet model\n","def build_alexnet_with_bn(input_shape=(227, 227, 3), num_classes=10):\n","    model = models.Sequential([\n","        layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Flatten(),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","    return model\n","\n","# function to train the model with different learning rates\n","def train_with_learning_rates(batch_size, learning_rates):\n","    for lr in learning_rates:\n","        print(f\"\\nTraining with Learning Rate = {lr}\\n\")\n","\n","        # Load the dataset with the given batch size\n","        train_dataset, validation_dataset = load_dataset(batch_size)\n","\n","        alexnet_model = build_alexnet_with_bn()\n","\n","        # compile the model with different learning rates\n","        optimizer = optimizers.Adam(learning_rate=lr)\n","        alexnet_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        # train the model\n","        history = alexnet_model.fit(\n","            train_dataset,\n","            validation_data=validation_dataset,\n","            epochs=5  # adjust the number of epochs as needed (fom 10 to 5 for testing purpose)\n","        )\n","\n","        # evaluate the model on the validation set\n","        val_loss, val_acc = alexnet_model.evaluate(validation_dataset, verbose=2)\n","        print(f\"Validation accuracy for Learning Rate = {lr}: {val_acc:.4f}\")\n","\n","        #plot the training and validation accuracy\n","        plt.plot(history.history['accuracy'], label='Training Accuracy')\n","        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.legend(loc='lower right')\n","        plt.title(f'Training and Validation Accuracy for Learning Rate = {lr}')\n","        plt.show()\n","\n","# testing with a fixed batch size and different learning rates\n","learning_rates = [0.1, 0.01, 0.001, 0.0001]\n","train_with_learning_rates(batch_size=32, learning_rates=learning_rates)\n"],"metadata":{"id":"WDmiPCcF4E_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["this section will test the RMSprop optimizer the adam optimier has already been applied through testing the seperate model."],"metadata":{"id":"OmAyTEQO4HI5"}},{"cell_type":"code","source":["# Install necessary libraries\n","!pip install patool==1.12\n","\n","# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, optimizers\n","import matplotlib.pyplot as plt\n","import patoolib\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Extract the zip file from Google Drive\n","#patoolib.extract_archive(\"/content/drive/MyDrive/Hood College/archive.zip\", outdir=\"/content/extracted_images\")\n","\n","# Define the directory containing the dataset\n","data_dir = '/content/extracted_images/leapGestRecog'\n","\n","# Function for loading the dataset\n","def load_dataset(batch_size):\n","    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.3,\n","        subset=\"training\",\n","        seed=123,\n","        image_size=(227, 227),\n","        batch_size=batch_size,\n","        label_mode='categorical'\n","    )\n","\n","    validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        data_dir,\n","        validation_split=0.3,\n","        subset=\"validation\",\n","        seed=123,\n","        image_size=(227, 227),\n","        batch_size=batch_size,\n","        label_mode='categorical'\n","    )\n","    return train_dataset, validation_dataset\n","\n","# build AlexNet model\n","def build_alexnet_with_bn(input_shape=(227, 227, 3), num_classes=10):\n","    model = models.Sequential([\n","        layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n","        layers.BatchNormalization(),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","\n","        layers.Flatten(),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","    return model\n","\n","# Function to train the model with RMSprop optimizer\n","def train_with_rmsprop(batch_size, learning_rate=0.001):\n","    print(f\"\\nTraining with RMSprop Optimizer and Learning Rate = {learning_rate}\\n\")\n","\n","    # Load the dataset with the given batch size\n","    train_dataset, validation_dataset = load_dataset(batch_size)\n","\n","    # Create the AlexNet\n","    alexnet_model = build_alexnet_with_bn()\n","\n","    # compile the model with RMSprop optimizer\n","    optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n","    alexnet_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    # train the model\n","    history = alexnet_model.fit(\n","        train_dataset,\n","        validation_data=validation_dataset,\n","        epochs=5\n","    )\n","\n","    # Evaluate the model on the validation set\n","    val_loss, val_acc = alexnet_model.evaluate(validation_dataset, verbose=2)\n","    print(f\"Validation accuracy: {val_acc:.4f}\")\n","\n","    # Plot the training and validation accuracy\n","    plt.plot(history.history['accuracy'], label='Training Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.title(f'Training and Validation Accuracy with RMSprop Optimizer')\n","    plt.show()\n","\n","# Test with a fixed batch size and RMSprop optimizer\n","train_with_rmsprop(batch_size=32, learning_rate=0.001)\n"],"metadata":{"id":"wfRP5Mnu90qJ"},"execution_count":null,"outputs":[]}]}