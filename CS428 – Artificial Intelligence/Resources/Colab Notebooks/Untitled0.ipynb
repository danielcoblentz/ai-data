{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPf6xiX4IjyLGr8Z2AitNuq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"vjrMbaXFexu8","executionInfo":{"status":"error","timestamp":1728250302402,"user_tz":240,"elapsed":167,"user":{"displayName":"Daniel Coblentz","userId":"11470372804666385483"}},"outputId":"b66f4864-b596-4d9c-d5fd-805707451340"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/archieve.zip'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-11ec4f1aee7e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Step 2: Extract the Zip File\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Extract the zip file to a folder named \"HandGestures\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HandGestures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/archieve.zip'"]}],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tensorflow.keras import layers, models\n","import zipfile\n","\n","# Step 1: Specify the Path to the Uploaded Zip File\n","zip_path = \"/content/archieve.zip\"  # Use the exact filename of the uploaded zip\n","\n","# Step 2: Extract the Zip File\n","# Extract the zip file to a folder named \"HandGestures\"\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(\"HandGestures\")\n","\n","# Step 3: Set the Data Directory Path After Extraction\n","# Update this based on the extracted folder structure\n","data_dir = \"HandGestures/leapGestRecog\"  # This should point to the main directory after extraction\n","\n","# Check the extracted folder structure to confirm\n","print(\"Directory Structure after Extraction:\")\n","print(os.listdir(data_dir))\n","\n","# Step 4: Load and Preprocess Images from Nested Folders\n","def load_images_from_directory(root_dir, target_size=(227, 227)):\n","    \"\"\"\n","    Traverse nested directories to load images and labels.\n","    Each class label is derived from the folder name (e.g., '01_palm' -> class label).\n","    \"\"\"\n","    images, labels = [], []\n","\n","    # Traverse through the main directory and its subdirectories\n","    for main_dir in os.listdir(root_dir):\n","        main_dir_path = os.path.join(root_dir, main_dir)  # e.g., \"HandGestures/leapGestRecog/01\"\n","        if not os.path.isdir(main_dir_path):\n","            continue\n","\n","        # Traverse each gesture folder inside each main directory (e.g., \"HandGestures/leapGestRecog/01/01_palm\")\n","        for gesture_dir in os.listdir(main_dir_path):\n","            gesture_dir_path = os.path.join(main_dir_path, gesture_dir)  # e.g., \"HandGestures/leapGestRecog/01/01_palm\"\n","\n","            if not os.path.isdir(gesture_dir_path):\n","                continue\n","\n","            # The gesture label can be derived from the folder name\n","            label = gesture_dir  # Use folder names as class labels (e.g., \"01_palm\")\n","\n","            # Traverse through each image in the gesture directory\n","            for img_name in os.listdir(gesture_dir_path):\n","                img_path = os.path.join(gesture_dir_path, img_name)\n","                try:\n","                    # Load the image, resize it to the target size, and convert to numpy array\n","                    img = load_img(img_path, target_size=target_size)\n","                    img_array = img_to_array(img)\n","                    images.append(img_array)\n","                    labels.append(label)  # Use folder name as label\n","                except Exception as e:\n","                    print(f\"Error loading image {img_path}: {e}\")\n","\n","    # Convert lists to numpy arrays\n","    return np.array(images, dtype='float32'), np.array(labels)\n","\n","# Load images and labels from the nested folder structure\n","images, labels = load_images_from_directory(data_dir)\n","\n","# Convert class labels to numerical values using a label encoder\n","from sklearn.preprocessing import LabelEncoder\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(labels)  # Convert string labels to integers\n","\n","# Normalize the images to the range [0, 1]\n","images = images / 255.0\n","\n","# Step 5: Split the Dataset into Training and Testing Sets\n","train_images, test_images, train_labels, test_labels = train_test_split(\n","    images, labels, test_size=0.3, random_state=42\n",")\n","\n","# Step 6: Define the AlexNet Model\n","def build_alexnet(input_shape=(227, 227, 3), num_classes=10):\n","    model = models.Sequential([\n","        layers.Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","        layers.Conv2D(256, (5, 5), padding='same', activation='relu'),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n","        layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n","        layers.MaxPooling2D((3, 3), strides=2),\n","        layers.Flatten(),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(4096, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation='softmax')  # Output layer for classification\n","    ])\n","    return model\n","\n","# Step 7: Build and Compile the Model\n","input_shape = (227, 227, 3)\n","num_classes = len(np.unique(labels))  # Number of unique class labels\n","alexnet_model = build_alexnet(input_shape=input_shape, num_classes=num_classes)\n","alexnet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Step 8: Train the Model\n","history = alexnet_model.fit(\n","    train_images, train_labels,  # Training data\n","    validation_data=(test_images, test_labels),  # Validation data\n","    epochs=15,  # Adjust the number of epochs based on performance\n","    batch_size=32,  # Adjust batch size if needed\n","    verbose=2  # Print progress during training\n",")\n","\n","# Step 9: Evaluate the Model\n","test_loss, test_acc = alexnet_model.evaluate(test_images, test_labels, verbose=2)\n","print(f\"\\nTest Accuracy: {test_acc}\")\n","\n","# Step 10: Visualize the Training and Validation Accuracy\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.title('Training and Validation Accuracy over Epochs')\n","plt.show()\n"]}]}