{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1NMp_AqChuSLLD8GmePCwTfbBaGRPWo8T","authorship_tag":"ABX9TyOFkcpG/Kly32uNYJe3IdPm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"Ums6mqnwsjb1","executionInfo":{"status":"error","timestamp":1728484667434,"user_tz":240,"elapsed":955,"user":{"displayName":"Daniel Coblentz","userId":"11470372804666385483"}},"outputId":"6450c074-f5d1-4042-d367-f529593287b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"NotFoundError","evalue":"Could not find directory /content/drive/MyDrive/Hood College/archive.zip","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-472092a90208>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Hood College/archive.zip'\u001b[0m  \u001b[0;31m# Replace with the path to your extracted data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m train_ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /content/drive/MyDrive/Hood College/archive.zip"]}],"source":["# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Load and preprocess the dataset\n","\n","IMG_WIDTH = 128  # Adjust according to your image size\n","IMG_HEIGHT = 128 # Adjust according to your image size\n","image_size = (IMG_WIDTH, IMG_HEIGHT)\n","BATCH_SIZE = 64  # Adjust as needed\n","data_dir = '/content/drive/MyDrive/Hood College/archive.zip'  # Replace with the path to your extracted data\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","    data_dir,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=image_size,\n","    batch_size=BATCH_SIZE\n",")\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","    data_dir,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=123,\n","    image_size=image_size,\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Load the MNIST dataset\n","# The dataset is split into training and test data\n","(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n","\n","# Reshape the data to add a channel dimension (needed for CNN)\n","# MNIST images are 28x28 pixels, and we need to add a channel dimension (1 for grayscale)\n","train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n","test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n","\n","# Normalize pixel values to be between 0 and 1 (originally between 0 and 255)\n","train_images = train_images.astype('float32') / 255\n","test_images = test_images.astype('float32') / 255\n","\n","# Build the CNN model\n","model = models.Sequential()\n","\n","# Add a convolutional layer with 32 filters, kernel size of 3x3, and ReLU activation\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","# Add a max-pooling layer with pool size of 2x2\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","# Add another convolutional layer with 64 filters and ReLU activation\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","# Add another max-pooling layer\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","# Add a final convolutional layer with 64 filters and ReLU activation\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","\n","# Flatten the output before feeding it into a dense layer\n","model.add(layers.Flatten())\n","\n","# Add a fully connected layer with 64 units and ReLU activation\n","model.add(layers.Dense(64, activation='relu'))\n","\n","# Add the output layer with 10 units (one for each digit) and softmax activation\n","model.add(layers.Dense(10, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(train_images, train_labels, epochs=5,\n","                    validation_data=(test_images, test_labels))\n","\n","# Plot training and validation accuracy\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# Evaluate the model on the test set\n","test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n","print(f'Test accuracy: {test_acc}')"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"k6H2bxH0tX2n"},"execution_count":null,"outputs":[]}]}